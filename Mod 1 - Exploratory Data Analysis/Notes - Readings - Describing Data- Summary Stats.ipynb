{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Descrbiing Data: Summary Statistics\n",
    "\n",
    "\n",
    "### Range = largest number - smallest number\n",
    "Not a good measure because of outliers\n",
    "\n",
    "### Variance:\n",
    "Measure of variation based on how far each point is from mean\n",
    "**Variance** = average of squared deviations from the mean\n",
    "**Standard Deviation** = sqrt(variance)\n",
    "\n",
    "Sample variance vs pop variance => divide by count vs divide by count - 1 (standard dev slightly larger)\n",
    "\n",
    "Why?\n",
    "First, recall again the whole notion of inference. We are interested in the population, but know only a sample. We are interested in knowing σ X 2  but—not knowing all the X i  in the population—are unable to use the formula for σ X 2 . Usually we will calculate s X 2  and use that as the basis for estimating σ X 2 . That means that we ought to calculate s X 2  in whatever way makes it the best estimator of σ X 2 . And using n X  – 1 instead of just n X  does that. To see why, notice the other substitution in the sample formula. Since we are not going to know μ X  the population mean either, we have substituted X–  instead. We are estimating μ X  with X–  as a first step in estimating σ X 2  with s X 2 . And we cannot pile estimate upon estimate without cost. In a sense, we have used up one bit of information in the first estimation. To see this more clearly, consider Figure 3.12. Column B shows a sample of five, taken from a larger population. Suppose the unknown mean of that population is 20. If we somehow knew that mean, we could use it as the basis for our deviations from the mean, as shown in column C. Notice that these sample deviations from the population mean do not sum to zero. This means that even the last one is providing independent information. We have n X  independent deviations from the population mean; hence, we have n X  independent squared deviations. It would make sense to find their average by dividing by n X . Now compare that with what we must do, given that we do not know the population mean. In this case, we must use the sample mean as the basis for our deviations from the mean, as shown in column F. Notice that these sample deviations from the sample mean do need to sum to zero. Hence, as soon as we know the first four deviations, we really know all f ive; the fifth deviation does not provide any independent information. We have only n X  – 1 independent deviations from the sample mean. Hence, we have n X  – 1 independent squared deviations. It makes sense to find their average by dividing by n X  – 1.\n",
    "\n",
    "\n",
    "**Coefficient of Variation**\n",
    "\n",
    "measures the standard deviation as a percentage of the mean - relative measure\n",
    "\n",
    "\n",
    "**Empirical Rule:** \n",
    "If data approx bell shaped:\n",
    "- 68% of data within 1 std dev of mean\n",
    "- 95% of data within 2 std dev of mean\n",
    "- virtually all data within 3 std dev of mean\n",
    "\n",
    "\n",
    "### 3.3 Measures of a single Categorical Variable\n",
    "best we can do with categorical data is to calculate proportions = frequency\n",
    "\n",
    "no numeric meaning with the data otherwise\n",
    "\n",
    "\n",
    "### 3.4 Measures of a Relationship\n",
    "Calculating a simple line regression\n",
    "figure 3.30\n",
    "\n",
    "The basic idea of inference remains the same here as in all the  previous examples. The X  cannot really help explain Y  unless the population slope, β , is nonzero. And we do not know the population slope. We have calculated the sample slope, b , in such a way as to make it the best possible estimate of β . However, the fact that it is our best possible estimate does not mean that it is correct. The true population value could still be zero. So the question becomes, how likely are we to get a sample  slope as big as 3 if the true population  slope is zero. You will learn, when we get to inference, how to make this calculation. Then, if you find it sufficiently unlikely, you will infer that the true value is not zero, and that X  does help explain some of the variation in Y ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
