{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## [Python Keras text classification]('https://realpython.com/python-keras-text-classification/')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentence    Wow... Loved this place.\nlabel                              1\nsource                          yelp\nName: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filepath_dict = {'yelp':   'data/yelp_labelled.txt',\n",
    "                 'amazon': 'data/amazon_cells_labelled.txt',\n",
    "                 'imdb':   'data/imdb_labelled.txt'}\n",
    "\n",
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = source  # Add another column filled with the source name\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# quick two sentence demo\n",
    "sentences = ['John likes ice cream.', 'John hates chocolate.']\n",
    "\n",
    "# Next, you can use the CountVectorizer provided by the scikit-learn library to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words:\n",
    "\n",
    "cv = CountVectorizer(min_df=0, lowercase=False)\n",
    "cv.fit(sentences)\n",
    "cv.vocabulary_\n",
    "\n",
    "# this vocabulary serves also as an index of each word. Now, you can take each sentence and get the word occurrences of the words based on the previous vocabulary. The vocabulary consists of all five words in our sentences, each representing one word in the vocabulary. When you take the previous two sentences and transform them with the CountVectorizer you will get a vector representing the count of each word of the sentence:\n",
    "\n",
    "cv.transform(sentences).toarray()\n",
    "\n",
    "# => array([[1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 0, 0]])\n",
    "#can see resulting feature vectors for each sentence based on prev vocab\n",
    "# Bag of Words model - common way to create vectors out of text\n",
    "# eacd document represented as a vector"
   ]
  },
  {
   "source": [
    "# Defining a baseline model\n",
    "simple model - used as comparison with more advanced models you want to test  \n",
    "this case - use baseline model to compare to the more advanced methods involving neural networks  \n",
    "\n",
    "first split data into training and test set  \n",
    "avoid overfitting - model trained too well on data and has just memorized training data  \n",
    "    would account for large accuracy in training data but low in testing data \n",
    "\n",
    "start with yelp set - extract sentences and labels  \n",
    "`.values` returns numpy array instead of pandas series object - easier to work with\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    "sentences = df_yelp['sentence'].values\n",
    "y = df_yelp['label'].values\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "source": [
    "### Here we will use again on the previous BOW model to vectorize the sentences.  \n",
    "### You can use again the CountVectorizer for this task. Since you might not have the testing data available during training, you can create the vocabulary using only the training data.  \n",
    "### Using this vocabulary, you can create the feature vectors for each sentence of the training and testing set:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<750x1714 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7368 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(sentences_train)\n",
    "\n",
    "X_train = cv.transform(sentences_train)\n",
    "X_test = cv.transform(sentences_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "source": [
    "750 = number of training samples \n",
    "1714 = size of vocabulary \n",
    "\n",
    "sparse matrix = data type optimized for matrices with only a few non-zero elements - only keeps track of the non-zero elements reducing memory load.  \n",
    "\n",
    "CV performs tokenization - seperates sentences into set of tokens as prev seen in the vocabulary  \n",
    "also removes punctuation and special chars, and apply other preprocessing to each word  \n",
    "\n",
    "can use a custom tokenizer from the NLTK library with the CV or any number of customizations\n",
    "\n",
    "## Classification  - logistic regression\n",
    "simple but powerful linear model - form of regression b/w 0 and 1 based on input feature vector  \n",
    "by specifying a cutoff value (0.5) regression model is used for Classification  \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy:  0.796\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "source": [
    "reached 79.6% accuracy  - now check other data sets\n",
    "\n",
    "this script perform and evaluates whole process for each dataset\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for yelp data: 0.7960\nAccuracy for amazon data: 0.7960\nAccuracy for imdb data: 0.7487\n"
     ]
    }
   ],
   "source": [
    "for source in df['source'].unique():\n",
    "    df_source = df[df['source'] == source]\n",
    "    sentences = df_source['sentence'].values\n",
    "    y = df_source['label'].values\n",
    "\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(sentences_train)\n",
    "    X_train = vectorizer.transform(sentences_train)\n",
    "    X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
   ]
  }
 ]
}