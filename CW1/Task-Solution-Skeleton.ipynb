{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and explore the data (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Description   Level_1 Level_2  \\\n",
       "count                                               10637     10649   10649   \n",
       "unique                                               9677        15      39   \n",
       "top     glory gorg col fing complet outfit express moo...  B092BA29   2D5A3   \n",
       "freq                                                   24       900     797   \n",
       "\n",
       "       Level_3  \n",
       "count    10649  \n",
       "unique      43  \n",
       "top       28A7  \n",
       "freq       797  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>Level_1</th>\n      <th>Level_2</th>\n      <th>Level_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10637</td>\n      <td>10649</td>\n      <td>10649</td>\n      <td>10649</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>9677</td>\n      <td>15</td>\n      <td>39</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>glory gorg col fing complet outfit express moo...</td>\n      <td>B092BA29</td>\n      <td>2D5A3</td>\n      <td>28A7</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>24</td>\n      <td>900</td>\n      <td>797</td>\n      <td>797</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data = pd.read_csv('product-cat-dataset.csv')\n",
    "df = pd.DataFrame(data)\n",
    "df.describe()\n",
    "\n",
    "# 15 level_1 classes # 39 level_2 classes # 43 level_3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         Description   Level_1 Level_2 Level_3\n",
       "0  gerb cap help keep littl on head cov warm day ...  09BF5150   C7E19    D06E\n",
       "1  newborn inf toddl boy hoody jacket oshkosh b g...  2CEC27F1   ADAD6    98CF\n",
       "2  tut ballet anym leap foxy fash ruffl tul toddl...  09BF5150   C7E19    D06E\n",
       "3  newborn inf toddl boy hoody jacket oshkosh b g...  2CEC27F1   ADAD6    98CF\n",
       "4  easy keep feel warm cozy inf toddl girl hoody ...  2CEC27F1   ADAD6    98CF"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>Level_1</th>\n      <th>Level_2</th>\n      <th>Level_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gerb cap help keep littl on head cov warm day ...</td>\n      <td>09BF5150</td>\n      <td>C7E19</td>\n      <td>D06E</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>newborn inf toddl boy hoody jacket oshkosh b g...</td>\n      <td>2CEC27F1</td>\n      <td>ADAD6</td>\n      <td>98CF</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tut ballet anym leap foxy fash ruffl tul toddl...</td>\n      <td>09BF5150</td>\n      <td>C7E19</td>\n      <td>D06E</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>newborn inf toddl boy hoody jacket oshkosh b g...</td>\n      <td>2CEC27F1</td>\n      <td>ADAD6</td>\n      <td>98CF</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>easy keep feel warm cozy inf toddl girl hoody ...</td>\n      <td>2CEC27F1</td>\n      <td>ADAD6</td>\n      <td>98CF</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Missing Data (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1063, 3435, 3459, 7763, 7797, 7805, 7817, 7868, 7945, 7971, 7997, 8013]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Check if data has missing values in the Description column\n",
    "# missing = df['Description'].isnull().index.tolist()\n",
    "\n",
    "missing_descriptions_indices = df[df['Description'].isnull()].index.tolist()\n",
    "missing_descriptions_indices\n",
    "#12 rows missing descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10637, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Remove missing descriptions rows from dataframe\n",
    "# df.dropna()\n",
    "df = df[df['Description'].notna()]\n",
    "df.shape\n",
    "#shape is 10637 rows which is 12 less than original 10649 so know we have dropped the correct amount of rows from the dataframe"
   ]
  },
  {
   "source": [
    "## Create subset of data to workwith as dataset too large"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8000, 4)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         Description   Level_1 Level_2 Level_3\n",
       "0  wrap fash warm frosty air ar zeroxpos versatil...  57164AC1   94728    5912\n",
       "1  exud feminin wear covington wom sleeveless blo...  2CEC27F1   BAE8A    2ABA\n",
       "2  prefect smokey look on conveny cas easy beauty...  3E1E0D78   9D9EE    05A0\n",
       "3  mak diap chang enjoy baby comfort warm wip sto...  4C3D8686   74974    62E8\n",
       "4  pres recip us fruit veget protein food soy pro...  B092BA29   375FE    1F61"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>Level_1</th>\n      <th>Level_2</th>\n      <th>Level_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wrap fash warm frosty air ar zeroxpos versatil...</td>\n      <td>57164AC1</td>\n      <td>94728</td>\n      <td>5912</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>exud feminin wear covington wom sleeveless blo...</td>\n      <td>2CEC27F1</td>\n      <td>BAE8A</td>\n      <td>2ABA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>prefect smokey look on conveny cas easy beauty...</td>\n      <td>3E1E0D78</td>\n      <td>9D9EE</td>\n      <td>05A0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mak diap chang enjoy baby comfort warm wip sto...</td>\n      <td>4C3D8686</td>\n      <td>74974</td>\n      <td>62E8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pres recip us fruit veget protein food soy pro...</td>\n      <td>B092BA29</td>\n      <td>375FE</td>\n      <td>1F61</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df = df.sample(n = 8000)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "# check shape to make sure correct transformation\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Classes where the number of instances is < 10 (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AAC8EE56    686\nB092BA29    667\n35E04739    663\n2CEC27F1    661\n57164AC1    657\nEFEF723B    596\n69286F45    591\n09BF5150    588\n4C3D8686    455\n3E1E0D78    450\n96F95EEC    429\n4513C920    422\nD410C91A    383\n90A8B052    379\n014303D1    373\nName: Level_1, dtype: int64\nNumber of Unique Level 1 Categories:  15\n"
     ]
    }
   ],
   "source": [
    "# Apply to Level_1 \n",
    "print(df.Level_1.value_counts())\n",
    "print('Number of Unique Level 1 Categories: ', df.Level_1.nunique())\n",
    "# 15 total categories for Level 1 - no classes with less than 10 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Unique Level 2 Categories:  36\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2D5A3    591\n",
       "ACD06    383\n",
       "C719A    360\n",
       "9D9EE    357\n",
       "914A1    348\n",
       "74974    344\n",
       "BAE8A    341\n",
       "5A8AB    339\n",
       "9B69F    338\n",
       "B2DB4    338\n",
       "94728    329\n",
       "375FE    328\n",
       "CB803    325\n",
       "390F1    321\n",
       "ADAD6    320\n",
       "C7E19    319\n",
       "7B638    314\n",
       "A04D3    295\n",
       "F4055    274\n",
       "7AED7    211\n",
       "02FA0    208\n",
       "77F62    162\n",
       "36080    134\n",
       "223B2    111\n",
       "E6162     93\n",
       "5E038     86\n",
       "E69F5     81\n",
       "31FED     67\n",
       "D5531     63\n",
       "F824F     50\n",
       "262E7     46\n",
       "915D4     32\n",
       "AF6B9     29\n",
       "6C6B1     26\n",
       "08960     19\n",
       "0864A     13\n",
       "Name: Level_2, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Apply to Level_2\n",
    "\n",
    "# create mask based on value counts\n",
    "mask_2 = df.Level_2.value_counts()\n",
    "# apply mask to dataset\n",
    "df = df[df['Level_2'].isin(mask_2.index[mask_2>9])]\n",
    "print('Number of Unique Level 2 Categories: ', df.Level_2.nunique())\n",
    "\n",
    "df.Level_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Unique Level 3 Categories:  38\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28A7    591\n",
       "33D1    383\n",
       "A0E2    360\n",
       "05A0    357\n",
       "D97D    348\n",
       "62E8    344\n",
       "2ABA    341\n",
       "AA6B    339\n",
       "21DA    338\n",
       "80C4    338\n",
       "1F61    328\n",
       "5912    328\n",
       "627D    325\n",
       "6856    321\n",
       "98CF    320\n",
       "D06E    319\n",
       "0F8B    314\n",
       "C5B4    295\n",
       "6539    211\n",
       "078B    208\n",
       "5AE1    162\n",
       "1F75    149\n",
       "C563    134\n",
       "7C00    125\n",
       "F213    111\n",
       "2E14     93\n",
       "6BE5     86\n",
       "DDD5     81\n",
       "6253     63\n",
       "7288     50\n",
       "29B3     46\n",
       "3DD3     45\n",
       "A2FA     32\n",
       "A104     29\n",
       "3AAD     26\n",
       "215F     22\n",
       "1000     19\n",
       "96B8     13\n",
       "Name: Level_3, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Apply to Level_3\n",
    "\n",
    "#create mask \n",
    "mask_3 = df.Level_3.value_counts()\n",
    "#apply mask\n",
    "df = df[df['Level_3'].isin(mask_3.index[mask_3>9])]\n",
    "\n",
    "print('Number of Unique Level 3 Categories: ', df.Level_3.nunique())\n",
    "\n",
    "# check value counts all above 10 instances\n",
    "df.Level_3.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's write a Function to Prepare Text (4 marks)\n",
    "We will apply it to our DataFrame later on\n",
    "\n",
    "* This function receives a text string and performs the following:\n",
    "* Convert text to lower case\n",
    "* Remove punctuation marks\n",
    "* Apply stemming using the popular Snowball or Porter Stemmer (optional)\n",
    "* Apply NGram Tokenisation\n",
    "* Return the tokenised text as a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\hugho\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import re\n",
    "snowball_stemmer = SnowballStemmer(language='english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# def my_preprocessor(text):\n",
    "#     text=text.lower() #lowercase text (done be default if don't use a custom preprocessor)\n",
    "#     text=re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "#     text=re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \",text) # normalize certain words\n",
    "    \n",
    "#     # stem words\n",
    "#     words=re.split(\"\\\\s+\",text)\n",
    "#     stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "#     return ' '.join(stemmed_words)\n",
    "\n",
    "# def scrub_words(text):\n",
    "#     \"\"\"Basic cleaning of texts.\"\"\"\n",
    "    \n",
    "#     # remove html markup\n",
    "#     text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "#     #remove non-ascii and digits\n",
    "#     text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "    \n",
    "#     #remove whitespace\n",
    "#     text=text.strip()\n",
    "#     return text\n",
    "\n",
    "def scrub_words(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'[_]','', text)\n",
    "    return text\n",
    "\n",
    "def process_text(text, n = 3):\n",
    "    # 1. Convert text to lower case and remove all punctuation\n",
    "    scrubbed_text =scrub_words(text)                \n",
    "    \n",
    "    # 2. Tokenize words\n",
    "    token_words = word_tokenize(scrubbed_text)      \n",
    "\n",
    "    #3. Apply stemming\n",
    "    stem_words = [snowball_stemmer.stem(w) for w in token_words] \n",
    "    \n",
    "    # 4. Apply Ngram Tokenisation\n",
    "    n_grams = ngrams(stem_words, n)                 \n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "def process_text2(text):\n",
    "    # 1. Convert text to lower case and remove all punctuation\n",
    "    scrubbed_text =scrub_words(text)                \n",
    "    \n",
    "    # 2. Tokenize words\n",
    "    token_words = word_tokenize(scrubbed_text)      \n",
    "\n",
    "    #3. Apply stemming\n",
    "    stem_words = [snowball_stemmer.stem(w) for w in token_words] \n",
    "    \n",
    "    # 4. Apply Ngram Tokenisation\n",
    "    return ' '.join(stem_words)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'here were test the processtext function result are as follow'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Here is an example function call\n",
    "process_text(\"Here we're testing the process_text function, results are as follows:\")\n",
    "process_text2(\"Here we're testing the process_text function, results are as follows:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's apply TF-IDF to extract features from plain text (10 marks)\n",
    "### Might take a while...\n",
    "### Here you apply the process_text function to the Description column of the data\n",
    "### Then you pass the results to the bag of words tranformer\n",
    "### See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df['bow'] = df['Description'].apply(lambda x: process_text(x, 3))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import chain"
   ]
  },
  {
   "source": [
    "## Use Count Vectorizer to get Bag of Words with ngram of 3 words "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# descriptions = df['Description'].values\n",
    "\n",
    "# cv = CountVectorizer(preprocessor=process_text2, ngram_range=(3,3), max_features=15000)\n",
    "# X = cv.fit_transform(descriptions)\n",
    "# count_vector = cv.transform(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape # => 10627, 10000 \n",
    "#10,627 documenst and vocab of 10,000\n",
    "# count_vector.shape\n"
   ]
  },
  {
   "source": [
    "## Calculate TFIDF Scores and add to DF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfTransformer()  \n",
    "# tfidf_transformer.fit(X)\n",
    "\n",
    "# df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(), columns=['idf_weights'])\n",
    "# df_idf.sort_values(by=['idf_weights'], ascending=False)\n",
    "\n",
    "# #compute tfidf scores\n",
    "# tf_idf_vector = tfidf_transformer.transform(count_vector)\n",
    "# feature_names = cv.get_feature_names()\n",
    "\n",
    "# #get tfidf vector for fist doc\n",
    "# first_descrip_vector = tf_idf_vector[0]\n",
    "\n",
    "# #print the scores for first description\n",
    "# tfidf_df_one = pd.DataFrame(first_descrip_vector.T.todense(), index=feature_names, columns=['tfidf'])\n",
    "# tfidf_df_one.sort_values(by=['tfidf'],ascending=False)\n",
    "\n",
    "# #print scores for all descriptions\n",
    "# tfidf_df = pd.DataFrame(tf_idf_vector.todense(), columns=feature_names)\n",
    "# tfidf_df.head()\n",
    "# tfidf_df.shape\n",
    "# #10,000 vocabs columns / 10627 descriptions rows\n"
   ]
  },
  {
   "source": [
    "## Use TFIDF Vectorizer to do same thing as combining Count Vectorizer and TFIDF Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(3,3), preprocessor=process_text2, max_features=15000)\n",
    "X = vectorizer.fit_transform(df['Description'].values)\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7994, 15000)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   00 ct tgw  000 first print  000 photo 10  01number pag 192bind  \\\n",
       "0        0.0              0.0           0.0                   0.0   \n",
       "1        0.0              0.0           0.0                   0.0   \n",
       "2        0.0              0.0           0.0                   0.0   \n",
       "3        0.0              0.0           0.0                   0.0   \n",
       "4        0.0              0.0           0.0                   0.0   \n",
       "\n",
       "   01number pag bind  02 05number pag  03 01number pag  04 01number pag  \\\n",
       "0                0.0              0.0              0.0              0.0   \n",
       "1                0.0              0.0              0.0              0.0   \n",
       "2                0.0              0.0              0.0              0.0   \n",
       "3                0.0              0.0              0.0              0.0   \n",
       "4                0.0              0.0              0.0              0.0   \n",
       "\n",
       "   05 01number pag  056 car bril  ...  zip pullov part  zip real fur  \\\n",
       "0              0.0           0.0  ...              0.0           0.0   \n",
       "1              0.0           0.0  ...              0.0           0.0   \n",
       "2              0.0           0.0  ...              0.0           0.0   \n",
       "3              0.0           0.0  ...              0.0           0.0   \n",
       "4              0.0           0.0  ...              0.0           0.0   \n",
       "\n",
       "   zip sid pocket  zip stash pocket  zip two front  ziploc brand dispo  \\\n",
       "0             0.0               0.0            0.0                 0.0   \n",
       "1             0.0               0.0            0.0                 0.0   \n",
       "2             0.0               0.0            0.0                 0.0   \n",
       "3             0.0               0.0            0.0                 0.0   \n",
       "4             0.0               0.0            0.0                 0.0   \n",
       "\n",
       "   zon afric also  zon ant col  zon babi us  zon provid gre  \n",
       "0             0.0          0.0          0.0             0.0  \n",
       "1             0.0          0.0          0.0             0.0  \n",
       "2             0.0          0.0          0.0             0.0  \n",
       "3             0.0          0.0          0.0             0.0  \n",
       "4             0.0          0.0          0.0             0.0  \n",
       "\n",
       "[5 rows x 15000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00 ct tgw</th>\n      <th>000 first print</th>\n      <th>000 photo 10</th>\n      <th>01number pag 192bind</th>\n      <th>01number pag bind</th>\n      <th>02 05number pag</th>\n      <th>03 01number pag</th>\n      <th>04 01number pag</th>\n      <th>05 01number pag</th>\n      <th>056 car bril</th>\n      <th>...</th>\n      <th>zip pullov part</th>\n      <th>zip real fur</th>\n      <th>zip sid pocket</th>\n      <th>zip stash pocket</th>\n      <th>zip two front</th>\n      <th>ziploc brand dispo</th>\n      <th>zon afric also</th>\n      <th>zon ant col</th>\n      <th>zon babi us</th>\n      <th>zon provid gre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 15000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "print(tfidf_df.shape)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "source": [
    "## Check to see if they are the same"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_df_one = pd.DataFrame(first_descrip_vector.T.todense(), index=feature_names, columns=['tfidf'])\n",
    "# # print(tfidf_df_one.sort_values(by=['tfidf'],ascending=False))\n",
    "\n",
    "# tfidf_df2_one = pd.DataFrame(X2[0].T.todense(), index=feature_names, columns=['tfidf'])\n",
    "# # print(tfidf_df2_one.sort_values(by=['tfidf'],ascending=False))\n",
    "\n",
    "\n",
    "# if ((tfidf_df_one.values == tfidf_df2_one.values).all()):\n",
    "#     print('Arrays are equal showing tfidf vectorizer returns same result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can use `.transform` on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of text file contents. Let's go ahead and check out how the bag-of-words counts for the entire corpus in a large, sparse matrix:\n",
    "\n",
    "After that you pass the result of the previous step to sklearn's TfidfTransformer  \n",
    "which will convert them into a feature matrix  \n",
    "See here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "\n",
    "The resulting matrix is in sparse format, we can transform it into dense  \n",
    "Code prepared for you so you can see what results look like\n",
    "\n",
    "This is an example result, the matrix will contain lots of zero values, that is expected.  \n",
    "Some values will be non-zero\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the Data is Ready for Classifier Usage\n",
    "\n",
    "### Split Data into Train and Test sets (4 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dfs before creating test and train datasets\n",
    "## REMOVE THESE COMMENTS BEFORE SUBMISSION\n",
    "# === not sure if actually need to do this ===\n",
    "#probably using same random_state paramater would result in same rows being used but whatever\n",
    "\n",
    "tfidf_df.reset_index(inplace=True, drop=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data = pd.concat([df, tfidf_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=1811)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, 4:]\n",
    "y_train = train.iloc[:, 0: 4]\n",
    "X_test = test.iloc[:, 4:]\n",
    "y_test = test.iloc[:, 0: 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to reset index in each dataframe (depends on you how you do things)\n",
    "# done for you to make it clearer\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to take classes as separate columns (depends on you how you do things)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class1_train = y_train['Level_1'].astype(str)\n",
    "class1_test = y_test['Level_1'].astype(str)\n",
    "\n",
    "class2_train = y_train['Level_2'].astype(str)\n",
    "class2_test = y_test['Level_2'].astype(str)\n",
    "\n",
    "class3_train = y_train['Level_3'].astype(str)\n",
    "class3_test = y_test['Level_3'].astype(str)\n",
    "\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# X_train_min_max = min_max_scaler.fit_transform(X_train)\n",
    "# X_test_min_max = min_max_scaler.fit_transform(X_test)\n",
    "\n",
    "# y_train[y_train['Level_1'].isnull()].index.tolist()\n",
    "# \n",
    "# missing_descriptions_indices = df[df['Description'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3988,)"
      ]
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training for the three levels (8 marks)\n",
    "\n",
    "naive bayes class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb = GuassianNB()\n",
    "# nb.fit(X, class1)\n",
    "# with open('level1.pk') as l1clf:\n",
    "#   pickle.dump(nb, l1clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12568/1973692754.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass1_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass1_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1404\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'processes'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m   1407\u001b[0m                                \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    756\u001b[0m             iprint = [-1, 50, 1, 100, 101][\n\u001b[0;32m    757\u001b[0m                 np.searchsorted(np.array([0, 1, 2, 3]), verbose)]\n\u001b[1;32m--> 758\u001b[1;33m             opt_res = optimize.minimize(\n\u001b[0m\u001b[0;32m    759\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    621\u001b[0m                                   **options)\n\u001b[0;32m    622\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[0;32m    624\u001b[0m                                 callback=callback, **options)\n\u001b[0;32m    625\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfunc\u001b[1;34m(x, *args)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_multi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0m_multinomial_loss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss_grad\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m     \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create and save model for level 1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, class1_train)\n",
    "score = classifier.score(X_test, class1_test)\n",
    "print('accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  0.6735459662288931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, class1_train)\n",
    "score = classifier.score(X_test, class1_test)\n",
    "print('Accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  0.724202626641651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "classifier = SGDClassifier().fit(X_train, class1_train)\n",
    "score = classifier.score(X_test, class1_test)\n",
    "print('Accuracy: ', score)\n",
    "\n",
    "with open('level1.pk', 'wb') as cls:\n",
    "    pickle.dump(classifier, cls)"
   ]
  },
  {
   "source": [
    "The Linear Model classifier had the greatest accuarcy of the three classifiers tested so using that going forward for model training of class 2 and 3."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Create and save models for level 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Level 1 Category:  3E1E0D78\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  3E1E0D78  SCORE:  0.85\n",
      "Level 1 Category:  2CEC27F1\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  2CEC27F1  SCORE:  0.905982905982906\n",
      "Level 1 Category:  D410C91A\n",
      "ERROR: ONLY ONE UNIQUE VALUE: ACD06  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 1 Category:  014303D1\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  014303D1  SCORE:  0.8831168831168831\n",
      "Level 1 Category:  AAC8EE56\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  AAC8EE56  SCORE:  0.8142857142857143\n",
      "Level 1 Category:  4513C920\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  4513C920  SCORE:  0.9191919191919192\n",
      "Level 1 Category:  B092BA29\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  B092BA29  SCORE:  0.55\n",
      "Level 1 Category:  96F95EEC\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  96F95EEC  SCORE:  0.8041237113402062\n",
      "Level 1 Category:  09BF5150\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  09BF5150  SCORE:  0.6666666666666666\n",
      "Level 1 Category:  35E04739\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  35E04739  SCORE:  0.8\n",
      "Level 1 Category:  57164AC1\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  57164AC1  SCORE:  0.751937984496124\n",
      "Level 1 Category:  EFEF723B\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  EFEF723B  SCORE:  0.8125\n",
      "Level 1 Category:  4C3D8686\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  4C3D8686  SCORE:  0.8354430379746836\n",
      "Level 1 Category:  69286F45\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 2D5A3  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 1 Category:  90A8B052\n",
      "\n",
      " Accuracy score for LVL 1 CAT:  90A8B052  SCORE:  0.9885057471264368\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['D410C91A', 'ACD06'], ['69286F45', '2D5A3']]"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# in this space you will be fitting models for the level 2 data \n",
    "#so you will want to split the test data into sub data for each classification e.g.\n",
    "\n",
    "#get unique level 1 categories\n",
    "level1cats = class1_train.unique()\n",
    "\n",
    "lvl1_cat_indexes = []\n",
    "lvl1_cat_indexes_test = []\n",
    "lvl1_unique = []\n",
    "\n",
    "for index, cat in enumerate(level1cats): \n",
    "    print('Level 1 Category: ', cat)\n",
    "    #get indexes for train data and test data\n",
    "    a = list(class1_train[class1_train == cat].index)\n",
    "    b = list(class1_test[class1_test == cat].index)\n",
    "    lvl1_cat_indexes.append(a)\n",
    "    lvl1_cat_indexes_test.append(b)\n",
    "\n",
    "    # some class 2 data only has one unique value for a particular level 1 category so can't create a model\n",
    "    if class2_train.loc[a].nunique() == 1:\n",
    "        unique_val = class2_train.loc[a].unique()[0]\n",
    "        print('ERROR: ONLY ONE UNIQUE VALUE:', unique_val, ' SO SKIP MODEL CREATION \\n')\n",
    "        #put values into array for prediction later\n",
    "        lvl1_unique.append([cat, unique_val])\n",
    "        continue\n",
    "    #create model with train data for unique level 1 category\n",
    "    classifier = SGDClassifier()\n",
    "    classifier.fit(X_train.loc[a], class2_train[a])\n",
    "\n",
    "    score = classifier.score(X_test.loc[b], class2_test[b])\n",
    "    print('\\n Accuracy score for LVL 1 CAT: ', cat, ' SCORE: ', score)\n",
    "\n",
    "    #save model\n",
    "    model_name = 'level2_' + cat + '.pk'\n",
    "    with open(model_name, 'wb') as cls: \n",
    "        pickle.dump(classifier, cls)\n",
    "    # limit = 5\n",
    "    # if index == limit: \n",
    "    #     break\n",
    "\n",
    "# cat 69286F45 unique val 2D5A3\n",
    "# D410C91A => ACD06\n",
    "lvl1_unique"
   ]
  },
  {
   "source": [
    "## Create and save models for level 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Level 2 Category:  9D9EE\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 05A0  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  ADAD6\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 98CF  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  ACD06\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 33D1  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  7AED7\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 6539  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  914A1\n",
      "ERROR: ONLY ONE UNIQUE VALUE: D97D  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  BAE8A\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 2ABA  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  31FED\n",
      "\n",
      " Accuracy score for LVL 2 CAT:  31FED  SCORE:  0.7692307692307693\n",
      "Level 2 Category:  5A8AB\n",
      "ERROR: ONLY ONE UNIQUE VALUE: AA6B  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  375FE\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 1F61  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  36080\n",
      "ERROR: ONLY ONE UNIQUE VALUE: C563  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  F824F\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 7288  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  B2DB4\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 21DA  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  C7E19\n",
      "ERROR: ONLY ONE UNIQUE VALUE: D06E  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  7B638\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 0F8B  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  02FA0\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 078B  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  74974\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 62E8  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  77F62\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 5AE1  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  9B69F\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 80C4  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  94728\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 5912  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  F4055\n",
      "\n",
      " Accuracy score for LVL 2 CAT:  F4055  SCORE:  0.6027397260273972\n",
      "Level 2 Category:  2D5A3\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 28A7  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  390F1\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 6856  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  E6162\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 2E14  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  5E038\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 6BE5  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  E69F5\n",
      "ERROR: ONLY ONE UNIQUE VALUE: DDD5  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  C719A\n",
      "ERROR: ONLY ONE UNIQUE VALUE: A0E2  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  CB803\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 627D  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  A04D3\n",
      "ERROR: ONLY ONE UNIQUE VALUE: C5B4  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  223B2\n",
      "ERROR: ONLY ONE UNIQUE VALUE: F213  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  262E7\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 29B3  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  6C6B1\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 3AAD  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  D5531\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 6253  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  915D4\n",
      "ERROR: ONLY ONE UNIQUE VALUE: A2FA  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  08960\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 1000  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  AF6B9\n",
      "ERROR: ONLY ONE UNIQUE VALUE: A104  SO SKIP MODEL CREATION \n",
      "\n",
      "Level 2 Category:  0864A\n",
      "ERROR: ONLY ONE UNIQUE VALUE: 96B8  SO SKIP MODEL CREATION \n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(34, 36)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# get unique level 2 categories\n",
    "level2cats = class2_train.unique()\n",
    "\n",
    "lvl2_cat_indexes = []\n",
    "lvl2_cat_indexes_test = []\n",
    "lvl2_unique = []\n",
    "for index, cat in enumerate(level2cats): \n",
    "    print('Level 2 Category: ', cat)\n",
    "    #get indexes for train data and test data\n",
    "    a = list(class2_train[class2_train == cat].index)\n",
    "    b = list(class2_test[class2_test == cat].index)\n",
    "\n",
    "    lvl2_cat_indexes.append(a)\n",
    "    lvl2_cat_indexes_test.append(b)\n",
    "\n",
    "    # some class 2 data only has one unique value for a particular level 1 category so can't create a model\n",
    "    if class3_train.loc[a].nunique() == 1:\n",
    "        unique_val = class3_train.loc[a].unique()[0]\n",
    "        print('ERROR: ONLY ONE UNIQUE VALUE:', unique_val, ' SO SKIP MODEL CREATION \\n')\n",
    "        #put values into array for prediction later\n",
    "        lvl2_unique.append([cat, unique_val])\n",
    "        continue\n",
    "    #create model with train data for unique level 2 category\n",
    "    classifier = SGDClassifier()\n",
    "    classifier.fit(X_train.loc[a], class3_train[a])\n",
    "    score = classifier.score(X_test.loc[b], class3_test[b])\n",
    "    print('\\n Accuracy score for LVL 2 CAT: ', cat, ' SCORE: ', score)\n",
    "\n",
    "    #save model\n",
    "    model_name = 'level3_' + cat + '.pk'\n",
    "    with open(model_name, 'wb') as cls: \n",
    "        pickle.dump(classifier, cls)\n",
    "    # limit = 2\n",
    "    # if index == limit: \n",
    "    #     break\n",
    "\n",
    "len(lvl2_unique), len(level2cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the test set (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Level1_Pred Level2_Pred Level3_Pred\n",
       "0    90A8B052         NaN         NaN\n",
       "1    B092BA29         NaN         NaN\n",
       "2    EFEF723B         NaN         NaN\n",
       "3    96F95EEC         NaN         NaN\n",
       "4    3E1E0D78         NaN         NaN"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Level1_Pred</th>\n      <th>Level2_Pred</th>\n      <th>Level3_Pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90A8B052</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B092BA29</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EFEF723B</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96F95EEC</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3E1E0D78</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# Creating an empty Dataframe with column names only (depends on you how you do things)\n",
    "results = pd.DataFrame(columns=['Level1_Pred', 'Level2_Pred', 'Level3_Pred'])\n",
    "\n",
    "## Here we reload the saved models and use them to predict the levels\n",
    "# load model for level 1 (done for you)\n",
    "with open('level1.pk', 'rb') as nb:\n",
    "    model = pickle.load(nb)\n",
    "\n",
    "## loop through the test data, predict level 1, then based on that predict level 2\n",
    "## and based on level 2 predict level 3 (you need to load saved models accordingly)\n",
    "level1_pred = model.predict(X_test)\n",
    "results['Level1_Pred'] = level1_pred\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['D410C91A', 'ACD06', '69286F45', '2D5A3']\n",
      "Level 1 Category:  3E1E0D78\n",
      "Level 1 Category:  2CEC27F1\n",
      "Level 1 Category:  D410C91A\n",
      "Unique Category - no model\n",
      "Level 1 Category:  014303D1\n",
      "Level 1 Category:  AAC8EE56\n",
      "Level 1 Category:  4513C920\n",
      "Level 1 Category:  B092BA29\n",
      "Level 1 Category:  96F95EEC\n",
      "Level 1 Category:  09BF5150\n",
      "Level 1 Category:  35E04739\n",
      "Level 1 Category:  57164AC1\n",
      "Level 1 Category:  EFEF723B\n",
      "Level 1 Category:  4C3D8686\n",
      "Level 1 Category:  69286F45\n",
      "Unique Category - no model\n",
      "Level 1 Category:  90A8B052\n"
     ]
    }
   ],
   "source": [
    "# for each category in level 1 predictions => use that plus the model for that category to predict level 2\n",
    "\n",
    "flat_lvl1_unique = [element for sublist in lvl1_unique for element in sublist]\n",
    "print(flat_lvl1_unique)\n",
    "for index, cat in enumerate(level1cats): \n",
    "    print('Level 1 Category: ', cat)\n",
    "    # get indexes\n",
    "    a = list(results[results['Level1_Pred']== cat].index)\n",
    "    # if cat is in the arrayof lvl1_unique => set predicted values to its pair\n",
    "    if cat in flat_lvl1_unique:\n",
    "        index = flat_lvl1_unique.index(cat)\n",
    "        predicted = flat_lvl1_unique[index+1]\n",
    "        print('Unique Category - no model')\n",
    "        results['Level2_Pred'].loc[a] =  predicted\n",
    "        continue\n",
    "    # get model\n",
    "    model_name = 'level2_' + cat + '.pk'\n",
    "    with open(model_name, 'rb') as nb:\n",
    "        model = pickle.load(nb)\n",
    "    results['Level2_Pred'].loc[a] =  model.predict(X_test.loc[a])\n",
    "    # print(model.predict(X_test.loc[a]))\n",
    "    # limit = 2\n",
    "    # if index == limit: \n",
    "    #     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Level 2 Category:  9D9EE\n",
      "Unique Category - no model\n",
      "Level 2 Category:  ADAD6\n",
      "Unique Category - no model\n",
      "Level 2 Category:  ACD06\n",
      "Unique Category - no model\n",
      "Level 2 Category:  7AED7\n",
      "Unique Category - no model\n",
      "Level 2 Category:  914A1\n",
      "Unique Category - no model\n",
      "Level 2 Category:  BAE8A\n",
      "Unique Category - no model\n",
      "Level 2 Category:  31FED\n",
      "Level 2 Category:  5A8AB\n",
      "Unique Category - no model\n",
      "Level 2 Category:  375FE\n",
      "Unique Category - no model\n",
      "Level 2 Category:  36080\n",
      "Unique Category - no model\n",
      "Level 2 Category:  F824F\n",
      "Unique Category - no model\n",
      "Level 2 Category:  B2DB4\n",
      "Unique Category - no model\n",
      "Level 2 Category:  C7E19\n",
      "Unique Category - no model\n",
      "Level 2 Category:  7B638\n",
      "Unique Category - no model\n",
      "Level 2 Category:  02FA0\n",
      "Unique Category - no model\n",
      "Level 2 Category:  74974\n",
      "Unique Category - no model\n",
      "Level 2 Category:  77F62\n",
      "Unique Category - no model\n",
      "Level 2 Category:  9B69F\n",
      "Unique Category - no model\n",
      "Level 2 Category:  94728\n",
      "Unique Category - no model\n",
      "Level 2 Category:  F4055\n",
      "Level 2 Category:  2D5A3\n",
      "Unique Category - no model\n",
      "Level 2 Category:  390F1\n",
      "Unique Category - no model\n",
      "Level 2 Category:  E6162\n",
      "Unique Category - no model\n",
      "Level 2 Category:  5E038\n",
      "Unique Category - no model\n",
      "Level 2 Category:  E69F5\n",
      "Unique Category - no model\n",
      "Level 2 Category:  C719A\n",
      "Unique Category - no model\n",
      "Level 2 Category:  CB803\n",
      "Unique Category - no model\n",
      "Level 2 Category:  A04D3\n",
      "Unique Category - no model\n",
      "Level 2 Category:  223B2\n",
      "Unique Category - no model\n",
      "Level 2 Category:  262E7\n",
      "Unique Category - no model\n",
      "Level 2 Category:  6C6B1\n",
      "Unique Category - no model\n",
      "Level 2 Category:  D5531\n",
      "Unique Category - no model\n",
      "Level 2 Category:  915D4\n",
      "Unique Category - no model\n",
      "Level 2 Category:  08960\n",
      "Unique Category - no model\n",
      "Level 2 Category:  AF6B9\n",
      "Unique Category - no model\n",
      "Level 2 Category:  0864A\n",
      "Unique Category - no model\n"
     ]
    }
   ],
   "source": [
    "# for each category in level 1 predictions => use that plus the model for that category to predict level 2\n",
    "\n",
    "flat_lvl2_unique = [element for sublist in lvl2_unique for element in sublist]\n",
    "# print(flat_lvl1_unique)\n",
    "for index, cat in enumerate(level2cats): \n",
    "    print('Level 2 Category: ', cat)\n",
    "    # get indexes\n",
    "    a = list(results[results['Level2_Pred']== cat].index)\n",
    "    # if cat is in the arrayof lvl1_unique => set predicted values to its pair\n",
    "    if cat in flat_lvl2_unique:\n",
    "        index = flat_lvl2_unique.index(cat)\n",
    "        predicted = flat_lvl2_unique[index+1]\n",
    "        print('Unique Category - no model')\n",
    "        results['Level3_Pred'].loc[a] =  predicted\n",
    "        continue\n",
    "    # get model\n",
    "    model_name = 'level3_' + cat + '.pk'\n",
    "    with open(model_name, 'rb') as nb:\n",
    "        model = pickle.load(nb)\n",
    "    results['Level3_Pred'].loc[a] =  model.predict(X_test.loc[a])\n",
    "    # print(model.predict(X_test.loc[a]))\n",
    "    # limit = 2\n",
    "    # if index == limit: \n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['Level3_Pred'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Level1_Pred Level2_Pred Level3_Pred\n",
       "0       90A8B052       08960        1000\n",
       "1       B092BA29       5A8AB        AA6B\n",
       "2       EFEF723B       CB803        627D\n",
       "3       96F95EEC       A04D3        C5B4\n",
       "4       3E1E0D78       E6162        2E14\n",
       "...          ...         ...         ...\n",
       "1594    014303D1       7AED7        6539\n",
       "1595    57164AC1       7B638        0F8B\n",
       "1596    D410C91A       ACD06        33D1\n",
       "1597    D410C91A       ACD06        33D1\n",
       "1598    B092BA29       5A8AB        AA6B\n",
       "\n",
       "[1599 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Level1_Pred</th>\n      <th>Level2_Pred</th>\n      <th>Level3_Pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90A8B052</td>\n      <td>08960</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B092BA29</td>\n      <td>5A8AB</td>\n      <td>AA6B</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EFEF723B</td>\n      <td>CB803</td>\n      <td>627D</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96F95EEC</td>\n      <td>A04D3</td>\n      <td>C5B4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3E1E0D78</td>\n      <td>E6162</td>\n      <td>2E14</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1594</th>\n      <td>014303D1</td>\n      <td>7AED7</td>\n      <td>6539</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>57164AC1</td>\n      <td>7B638</td>\n      <td>0F8B</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>D410C91A</td>\n      <td>ACD06</td>\n      <td>33D1</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>D410C91A</td>\n      <td>ACD06</td>\n      <td>33D1</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>B092BA29</td>\n      <td>5A8AB</td>\n      <td>AA6B</td>\n    </tr>\n  </tbody>\n</table>\n<p>1599 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level1_Pred</th>\n",
       "      <th>Level2_Pred</th>\n",
       "      <th>Level3_Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2CEC27F1</td>\n",
       "      <td>BAE8A</td>\n",
       "      <td>2ABA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2CEC27F1</td>\n",
       "      <td>BAE8A</td>\n",
       "      <td>2ABA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09BF5150</td>\n",
       "      <td>C7E19</td>\n",
       "      <td>D06E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4513C920</td>\n",
       "      <td>F4055</td>\n",
       "      <td>7C00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4513C920</td>\n",
       "      <td>F4055</td>\n",
       "      <td>7C00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>09BF5150</td>\n",
       "      <td>262E7</td>\n",
       "      <td>29B3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>69286F45</td>\n",
       "      <td>2D5A3</td>\n",
       "      <td>28A7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>3E1E0D78</td>\n",
       "      <td>9D9EE</td>\n",
       "      <td>05A0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>57164AC1</td>\n",
       "      <td>7B638</td>\n",
       "      <td>0F8B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>90A8B052</td>\n",
       "      <td>C719A</td>\n",
       "      <td>A0E2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2126 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Level1_Pred Level2_Pred Level3_Pred\n",
       "0       2CEC27F1       BAE8A        2ABA\n",
       "1       2CEC27F1       BAE8A        2ABA\n",
       "2       09BF5150       C7E19        D06E\n",
       "3       4513C920       F4055        7C00\n",
       "4       4513C920       F4055        7C00\n",
       "...          ...         ...         ...\n",
       "2121    09BF5150       262E7        29B3\n",
       "2122    69286F45       2D5A3        28A7\n",
       "2123    3E1E0D78       9D9EE        05A0\n",
       "2124    57164AC1       7B638        0F8B\n",
       "2125    90A8B052       C719A        A0E2\n",
       "\n",
       "[2126 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## After you add the predictions to the results dataframe\n",
    "## they should look like this\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Accuracy on each level (4 marks)\n",
    "Now you have the predictions for each level (in the test data), and you also have the actual levels, you can compute the accurcay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LEVEL 1 ACCURACY:  0.724202626641651\n"
     ]
    }
   ],
   "source": [
    "# Level 1 accuracy\n",
    "print('LEVEL 1 ACCURACY: ', accuracy_score(y_test['Level_1'], level1_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LEVEL 2 ACCURACY:  0.6216385240775485\n"
     ]
    }
   ],
   "source": [
    "# Level 2 accuracy\n",
    "print('LEVEL 2 ACCURACY: ', accuracy_score(y_test['Level_2'], results['Level2_Pred']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LEVEL 3 ACCURACY:  0.6128830519074422\n"
     ]
    }
   ],
   "source": [
    "# Level 3 accuracy\n",
    "print('LEVEL 3 ACCURACY: ', accuracy_score(y_test['Level_3'], results['Level3_Pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python396jvsc74a57bd0b874ae6ca3bf8380d01ff748c279caa87bf096af67f0e2a4c58ac8b205c3f032",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "b874ae6ca3bf8380d01ff748c279caa87bf096af67f0e2a4c58ac8b205c3f032"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}